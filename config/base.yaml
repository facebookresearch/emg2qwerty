defaults:
  - transforms: log_mel
  - model: tds_conv_ctc
  - user: generic

seed: 1501
batch_size: 64
num_workers: 10

trainer:
  # TODO: ddp with hydra launcher multirun leads to some deadlock
  accelerator: ddp_spawn
  num_nodes: 1
  gpus: 2
  max_epochs: 3

callbacks:
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_CER
    mode: min
    save_last: True
    verbose: True

dataset:
  root: /private/home/viswanath/emg2qwerty-data-2021-06-24

hydra:
  run:
    dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}_${hydra.job.override_dirname}
  job:
    name: emg2qwerty
    config:
      override_dirname:
        exclude_keys:
          - cluster
